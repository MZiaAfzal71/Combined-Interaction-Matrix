{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/GZyGKgu5Scfi4/5IDkFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MZiaAfzal71/Edge-Aware-GNN/blob/main/Models/Descriptor_Augmented_Edge_Aware_GNN_for_ESOL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptor-Augmented Edge-Aware GNN for ESOL\n",
        "\n",
        "This notebook implements a **Descriptor-Augmented Edge-Aware Graph Neural Network (DA-EA-GNN)**\n",
        "for predicting aqueous solubility on the **ESOL (Delaney) dataset**.\n",
        "\n",
        "Each molecule is represented using a **hybrid molecular representation** that combines:\n",
        "- A **graph-based representation** derived from its **SMILES string**, and\n",
        "- **Global physicochemical descriptors** computed using **RDKit**\n",
        "\n",
        "In the molecular graph:\n",
        "- **Nodes** correspond to atoms with chemically meaningful atom features\n",
        "- **Edges** correspond to bonds with explicit bond features\n",
        "- Message passing is performed using **edge-aware GNN layers** to incorporate bond information\n",
        "\n",
        "In addition to the graph representation:\n",
        "- A set of **RDKit molecular descriptors** is computed for each molecule\n",
        "- All descriptors, or a chemically relevant subset tailored for solubility prediction, are used\n",
        "- Descriptor features are normalized and fused with graph-level embeddings\n",
        "\n",
        "In this notebook:\n",
        "- Both **structure-level (atom‚Äìbond)** and **molecule-level (descriptor)** information are used\n",
        "- Training and evaluation are performed using:\n",
        "  - **Repeated cross-validation**, and\n",
        "  - A **Bemis‚ÄìMurcko scaffold-based split** to assess generalization to unseen chemical scaffolds\n",
        "- Results are compared with the structure-only edge-aware GNN to evaluate the impact of descriptor fusion\n",
        "\n",
        "This setup allows us to study the effect of incorporating global physicochemical information\n",
        "alongside edge-aware message passing under both random and chemically realistic evaluation\n",
        "protocols for molecular property prediction.\n",
        "\n",
        "The notebook is designed to be:\n",
        "- **Reproducible**\n",
        "- **Interpretable**\n",
        "- **Focused on understanding representation choices**\n",
        "\n",
        "The ESOL dataset contains 1,128 small molecules with experimentally measured aqueous solubility\n",
        "values and serves as a standard benchmark for evaluating molecular machine learning models.\n"
      ],
      "metadata": {
        "id": "Xd3ySVIzQoiH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RvEcRfaPXLD"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£ Fetch data\n",
        "!git clone https://github.com/MZiaAfzal71/Edge-Aware-GNN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ Change current/working directory\n",
        "%cd Edge-Aware-GNN/ESOL\\ Dataset"
      ],
      "metadata": {
        "id": "g2vKKgZ_SyKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3Ô∏è‚É£ Install rdkit and PyG\n",
        "!pip install rdkit torch_geometric"
      ],
      "metadata": {
        "id": "ghQBihVfS32V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  4Ô∏è‚É£ Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GINEConv, AttentionalAggregation\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "# from rdkit.Chem.Scaffolds import MurckoScaffold\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold, train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "EsmPGTw8S8Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£ Set random seeds for reproducibility across Python, NumPy, and PyTorch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "J6T6ZKsiY_N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6Ô∏è‚É£ Utility functions to convert SMILES strings into graph representations with atom and bond features\n",
        "\n",
        "ELECTRONEGATIVITY = {\n",
        "    1: 2.20, 6: 2.55, 7: 3.04, 8: 3.44,\n",
        "    9: 3.98, 15: 2.19, 16: 2.58,\n",
        "    17: 3.16, 35: 2.96, 53: 2.66\n",
        "}\n",
        "\n",
        "def atom_features(atom):\n",
        "    Z = atom.GetAtomicNum()\n",
        "\n",
        "    hyb = atom.GetHybridization()\n",
        "    hyb_onehot = [\n",
        "        hyb == Chem.rdchem.HybridizationType.SP,\n",
        "        hyb == Chem.rdchem.HybridizationType.SP2,\n",
        "        hyb == Chem.rdchem.HybridizationType.SP3\n",
        "    ]\n",
        "\n",
        "    chiral = atom.GetChiralTag()\n",
        "    chiral_onehot = [\n",
        "        chiral == Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "        chiral == Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW\n",
        "    ]\n",
        "\n",
        "    return [\n",
        "        Z,\n",
        "        # atom.GetMass(),\n",
        "        atom.GetDegree(),\n",
        "        atom.GetTotalValence(),\n",
        "        atom.GetTotalNumHs(),\n",
        "        atom.GetFormalCharge(),\n",
        "        float(atom.GetIsAromatic()),\n",
        "        float(atom.IsInRing()),\n",
        "        *hyb_onehot,\n",
        "        # *chiral_onehot,\n",
        "        ELECTRONEGATIVITY.get(Z, 0.0)\n",
        "    ]\n",
        "\n",
        "def bond_features(bond):\n",
        "    bt = bond.GetBondType()\n",
        "\n",
        "    stereo = bond.GetStereo()\n",
        "    stereo_onehot = [\n",
        "        stereo == Chem.rdchem.BondStereo.STEREOE,\n",
        "        stereo == Chem.rdchem.BondStereo.STEREOZ\n",
        "    ]\n",
        "\n",
        "    return [\n",
        "        bt == Chem.rdchem.BondType.SINGLE,\n",
        "        bt == Chem.rdchem.BondType.DOUBLE,\n",
        "        bt == Chem.rdchem.BondType.TRIPLE,\n",
        "        bt == Chem.rdchem.BondType.AROMATIC,\n",
        "        float(bond.GetIsConjugated()),\n",
        "        float(bond.IsInRing()),\n",
        "        # float(bond.IsRotor()),\n",
        "        # float(bond.GetIsAmide()),\n",
        "        # *stereo_onehot\n",
        "    ]\n",
        "\n",
        "def smiles_to_graph(smiles, y=None):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    x = [atom_features(atom) for atom in mol.GetAtoms()]\n",
        "\n",
        "    edge_index, edge_attr = [], []\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        bf = bond_features(bond)\n",
        "\n",
        "        edge_index += [[i, j], [j, i]]\n",
        "        edge_attr += [bf, bf]\n",
        "\n",
        "    data = Data(\n",
        "        x=torch.tensor(x, dtype=torch.float),\n",
        "        edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous(),\n",
        "        edge_attr=torch.tensor(edge_attr, dtype=torch.float)\n",
        "    )\n",
        "\n",
        "    if y is not None:\n",
        "        data.y = torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "5ODT13mFWuW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7Ô∏è‚É£ PyTorch Dataset Class\n",
        "\n",
        "class MoleculeDataset(Dataset):\n",
        "    def __init__(self, df, desc, smiles_col, target_col, scaler=None, fit_scaler=False):\n",
        "        self.smiles = df[smiles_col].values\n",
        "        self.targets = df[target_col].values.astype(np.float32)\n",
        "\n",
        "        # Compute descriptors\n",
        "        descriptors = desc\n",
        "\n",
        "        # Normalize descriptors\n",
        "        if fit_scaler:\n",
        "            self.scaler = RobustScaler()\n",
        "            self.descriptors = self.scaler.fit_transform(descriptors)\n",
        "            self.descriptors = np.clip(self.descriptors, -5, 5)\n",
        "        elif scaler is not None:\n",
        "            self.scaler = scaler\n",
        "            self.descriptors = self.scaler.transform(descriptors)\n",
        "            self.descriptors = np.clip(self.descriptors, -5, 5)\n",
        "        else:\n",
        "            self.scaler = None\n",
        "            self.descriptors = np.clip(descriptors, -5, 5)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        graph = smiles_to_graph(self.smiles[idx])\n",
        "        graph.y = torch.tensor([self.targets[idx]], dtype=torch.float)\n",
        "\n",
        "        desc = torch.tensor(self.descriptors[idx], dtype=torch.float)\n",
        "        return graph, desc"
      ],
      "metadata": {
        "id": "VmsYtVcfYpKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8Ô∏è‚É£ Collate Function (Required for PyG + Descriptors)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    graphs, descs = zip(*batch)\n",
        "    batch_graph = Data.from_data_list(graphs)\n",
        "    batch_desc = torch.stack(descs)\n",
        "    return batch_graph, batch_desc"
      ],
      "metadata": {
        "id": "SQKMJlLol5O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9Ô∏è‚É£ GINE-based graph neural network with attentional readout for molecular graphs\n",
        "\n",
        "class SimpleGINE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_dim,\n",
        "        edge_dim,\n",
        "        hidden_dim=128,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        def make_mlp():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            )\n",
        "\n",
        "        self.node_emb = nn.Linear(node_dim, hidden_dim)\n",
        "\n",
        "        self.conv1 = GINEConv(make_mlp(), edge_dim=edge_dim)\n",
        "        self.conv2 = GINEConv(make_mlp(), edge_dim=edge_dim)\n",
        "        self.conv3 = GINEConv(make_mlp(), edge_dim=edge_dim)\n",
        "\n",
        "        # ---- Attention readout ----\n",
        "        self.readout = AttentionalAggregation(\n",
        "            gate_nn=nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, 1)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        x = self.node_emb(x)\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
        "\n",
        "        return self.readout(x, batch)"
      ],
      "metadata": {
        "id": "S2PV6swFYrbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîü GNN + Descriptor Fusion Model\n",
        "\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, g, d):\n",
        "        gate = self.gate(torch.cat([g, d], dim=1))\n",
        "        return gate * g + (1.0 - gate) * d"
      ],
      "metadata": {
        "id": "GYmtYdz8mGSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£1Ô∏è‚É£ Descriptor-Augmented Edge-aware graph neural network (DA_EAGNN) based on GINE for molecular property prediction\n",
        "\n",
        "class DA_EAGNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_dim,\n",
        "        desc_dim,\n",
        "        edge_dim,\n",
        "        hidden_dim=128,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gnn = SimpleGINE(\n",
        "            node_dim=node_dim,\n",
        "            edge_dim=edge_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.desc_net = nn.Sequential(\n",
        "            nn.Linear(desc_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.fusion = GatedFusion(hidden_dim)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, graphs, desc):\n",
        "        g = self.gnn(\n",
        "            graphs.x,\n",
        "            graphs.edge_index,\n",
        "            graphs.edge_attr,\n",
        "            graphs.batch\n",
        "        )\n",
        "\n",
        "        d = self.desc_net(desc)\n",
        "\n",
        "        h = self.fusion(g, d)\n",
        "        return self.head(h).squeeze(-1)\n"
      ],
      "metadata": {
        "id": "_u6Y9kozYtRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£2Ô∏è‚É£ Utility function for repeated k-fold cross-validation with PyG data loaders\n",
        "\n",
        "def run_repeated_kfold_cv(\n",
        "    df,\n",
        "    descriptors,\n",
        "    smiles_col=\"smiles\",\n",
        "    target_col=\"target\",\n",
        "    n_splits=5,\n",
        "    n_repeats=5,\n",
        "    batch_size=32,\n",
        "    seed=42\n",
        "):\n",
        "    rkf = RepeatedKFold(\n",
        "        n_splits=n_splits,\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    split_id = 0\n",
        "\n",
        "    for train_idx, val_idx in rkf.split(df):\n",
        "        repeat = split_id // n_splits\n",
        "        fold   = split_id % n_splits\n",
        "\n",
        "        train_df = df.iloc[train_idx]\n",
        "        val_df   = df.iloc[val_idx]\n",
        "\n",
        "        train_desc = descriptors[train_idx]\n",
        "        val_desc = descriptors[val_idx]\n",
        "\n",
        "        # ---- Train dataset (fit scaler) ----\n",
        "        train_dataset = MoleculeDataset(\n",
        "            train_df,\n",
        "            train_desc,\n",
        "            smiles_col,\n",
        "            target_col,\n",
        "            fit_scaler=True\n",
        "        )\n",
        "\n",
        "        # ---- Validation dataset (reuse scaler) ----\n",
        "        val_dataset = MoleculeDataset(\n",
        "            val_df,\n",
        "            val_desc,\n",
        "            smiles_col,\n",
        "            target_col,\n",
        "            scaler=train_dataset.scaler\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "        yield repeat, fold, train_loader, val_loader\n",
        "\n",
        "        split_id += 1\n"
      ],
      "metadata": {
        "id": "T4SowaKqYwuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£3Ô∏è‚É£ # Training utility with early stopping, learning rate scheduling, and model checkpointing\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        device,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-4,\n",
        "        patience=10,\n",
        "        max_epochs=100\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.patience = patience\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "        # self.criterion = nn.SmoothL1Loss(beta=1.0)\n",
        "\n",
        "        self.optimizer = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode=\"min\",\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "        self.best_model_state = None\n",
        "        self.history = {\n",
        "            \"train_loss\": [],\n",
        "            \"val_loss\": []\n",
        "        }\n",
        "\n",
        "\n",
        "    def _train_one_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for graphs, desc in train_loader:\n",
        "            graphs = graphs.to(self.device)\n",
        "            desc = desc.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            preds = self.model(graphs, desc)\n",
        "            loss = self.criterion(preds, graphs.y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        return running_loss / len(train_loader)\n",
        "\n",
        "\n",
        "    def _evaluate(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        preds_all, targets_all = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for graphs, desc in loader:\n",
        "                graphs = graphs.to(self.device)\n",
        "                desc = desc.to(self.device)\n",
        "\n",
        "                preds = self.model(graphs, desc)\n",
        "                loss = self.criterion(preds, graphs.y)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                preds_all.append(preds.cpu())\n",
        "                targets_all.append(graphs.y.cpu())\n",
        "\n",
        "        return (\n",
        "            total_loss / len(loader),\n",
        "            torch.cat(preds_all),\n",
        "            torch.cat(targets_all)\n",
        "        )\n",
        "\n",
        "\n",
        "    def fit(self, train_loader, val_loader, verbose=True):\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "\n",
        "            train_loss = self._train_one_epoch(train_loader)\n",
        "            val_loss, _, _ = self._evaluate(val_loader)\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            self.history[\"train_loss\"].append(train_loss)\n",
        "            self.history[\"val_loss\"].append(val_loss)\n",
        "\n",
        "            if verbose:\n",
        "                print(\n",
        "                    f\"Epoch {epoch:03d} | \"\n",
        "                    f\"Train: {train_loss:.4f} | \"\n",
        "                    f\"Val: {val_loss:.4f}\"\n",
        "                )\n",
        "\n",
        "            # ---- Best Model Tracking ----\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_model_state = copy.deepcopy(\n",
        "                    self.model.state_dict()\n",
        "                )\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            # ---- Early Stopping ----\n",
        "            if patience_counter >= self.patience:\n",
        "                if verbose:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Restore best model\n",
        "        self.model.load_state_dict(self.best_model_state)\n",
        "\n",
        "        return self.model, self.best_val_loss\n",
        "\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        return self._evaluate(test_loader)\n",
        "\n",
        "\n",
        "    def save_best_model(self, path):\n",
        "        torch.save(self.best_model_state, path)\n",
        "\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n"
      ],
      "metadata": {
        "id": "CsqvNJElyB3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£4Ô∏è‚É£ Calculates all available RDKit descriptors for a given SMILES string.\n",
        "\n",
        "def rdkit_descriptors_from_smiles(smiles):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles : list of smiles strings\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with descriptor names as columns.\n",
        "        Returns NaN values if SMILES is invalid.\n",
        "    \"\"\"\n",
        "    # Get descriptor names and functions\n",
        "    rdkit_descs = []\n",
        "\n",
        "    desc_list = Descriptors.descList\n",
        "    desc_names = [name for name, _ in desc_list]\n",
        "\n",
        "    # Initialize output with NaNs\n",
        "    # values = [np.nan] * len(desc_names)\n",
        "\n",
        "    for sm in tqdm(smiles, total=len(smiles)):\n",
        "      sm_descs = []\n",
        "      mol = Chem.MolFromSmiles(sm)\n",
        "      if mol is None:\n",
        "          rdkit_descs.append([]*len(desc_names))\n",
        "          continue\n",
        "\n",
        "      for _, func in desc_list:\n",
        "          try:\n",
        "              sm_descs.append(func(mol))\n",
        "          except Exception:\n",
        "              sm_descs.append(np.nan)\n",
        "\n",
        "      rdkit_descs.append(sm_descs)\n",
        "\n",
        "    return pd.DataFrame(rdkit_descs, columns=desc_names)\n"
      ],
      "metadata": {
        "id": "xzPfo8Dxk59x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£5Ô∏è‚É£ Load dataset, standardize target variable, and prepare data for modeling\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "file_path = \"delaney-processed-scaffold.csv\"\n",
        "smiles_col = \"smiles\"\n",
        "target_col = \"measured log solubility in mols per litre\"\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "y = df[target_col]\n",
        "\n",
        "y_mean = y.mean()\n",
        "y_std  = y.std()\n",
        "\n",
        "y_scaled = (y - y_mean) / y_std\n",
        "\n",
        "df_data = pd.concat([df[smiles_col], y_scaled], axis=1)\n",
        "\n",
        "rdkit_descriptors = rdkit_descriptors_from_smiles(df[smiles_col])\n",
        "variance_df = rdkit_descriptors.var()\n",
        "zero_var_columns = variance_df[variance_df == 0].index.tolist()\n",
        "cleaned_rdkit_descs = rdkit_descriptors.drop(columns = zero_var_columns)\n",
        "X = cleaned_rdkit_descs.values\n"
      ],
      "metadata": {
        "id": "VkLBJeGrzo5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£6Ô∏è‚É£ Run repeated k-fold cross-validation training and record best validation loss per fold\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "for repeat, fold, train_loader, val_loader in run_repeated_kfold_cv(\n",
        "    df_data,\n",
        "    X,\n",
        "    smiles_col=smiles_col,\n",
        "    target_col=target_col,\n",
        "    n_splits=5,\n",
        "    n_repeats=5\n",
        "):\n",
        "    print(f\"\\n===== Repeat {repeat + 1} | Fold {fold + 1} =====\")\n",
        "\n",
        "    model = DA_EAGNN(\n",
        "        node_dim=11,\n",
        "        desc_dim=X.shape[1],\n",
        "        edge_dim=6,\n",
        "        hidden_dim=128,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        device=device,\n",
        "        lr=1e-3,\n",
        "        patience=20,\n",
        "        max_epochs=150\n",
        "    )\n",
        "\n",
        "    model, best_val_loss = trainer.fit(\n",
        "        train_loader,\n",
        "        val_loader\n",
        "    )\n",
        "\n",
        "    _, train_preds, train_targets = trainer.test(train_loader)\n",
        "    _, val_preds, val_targets = trainer.test(val_loader)\n",
        "\n",
        "    train_preds_true = (train_preds * y_std + y_mean).numpy()\n",
        "    train_targets_true = (train_targets * y_std + y_mean).numpy()\n",
        "\n",
        "    val_preds_true = (val_preds * y_std + y_mean).numpy()\n",
        "    val_targets_true = (val_targets * y_std + y_mean).numpy()\n",
        "\n",
        "    train_rmse = np.sqrt(mean_squared_error(\n",
        "        train_preds_true,\n",
        "        train_targets_true\n",
        "    ))\n",
        "\n",
        "    train_r2 = r2_score(\n",
        "        train_preds_true,\n",
        "        train_targets_true\n",
        "    )\n",
        "\n",
        "    train_mae = mean_absolute_error(\n",
        "        train_preds_true,\n",
        "        train_targets_true\n",
        "    )\n",
        "\n",
        "    val_rmse = np.sqrt(mean_squared_error(\n",
        "        val_preds_true,\n",
        "        val_targets_true\n",
        "    ))\n",
        "\n",
        "    val_r2 = r2_score(\n",
        "        val_preds_true,\n",
        "        val_targets_true\n",
        "    )\n",
        "\n",
        "    val_mae = mean_absolute_error(\n",
        "        val_preds_true,\n",
        "        val_targets_true\n",
        "    )\n",
        "\n",
        "    fold_results.append({\n",
        "        \"repeat\": repeat + 1,\n",
        "        \"fold\": fold + 1,\n",
        "        \"best_train_rmse\": train_rmse,\n",
        "        \"best_train_r2\": train_r2,\n",
        "        \"best_train_mae\": train_mae,\n",
        "        \"best_val_rmse\": val_rmse,\n",
        "        \"best_val_r2\": val_r2,\n",
        "        \"best_val_mae\": val_mae\n",
        "    })\n",
        "\n",
        "    print(\n",
        "        f\"Repeat {repeat + 1} | Fold {fold + 1} | \"\n",
        "        f\"Best Val Loss: {best_val_loss:.4f}\"\n",
        "    )\n",
        "\n",
        "fold_results_df = pd.DataFrame(fold_results)\n",
        "fold_results_df.to_csv(\"Fold results descriptor augmented edge aware GNN.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "eC-s3Z7vx51-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£7Ô∏è‚É£ Train an ensemble of descriptor-aware edge-aware GNN models using scaffold-based splits and report train/validation metrics\n",
        "\n",
        "def train_ensemble_scaffold(\n",
        "    df,\n",
        "    X,\n",
        "    split_col,\n",
        "    device,\n",
        "    smiles_col=smiles_col,\n",
        "    target_col=target_col,\n",
        "    y_mean=y_mean,\n",
        "    y_std=y_std,\n",
        "    batch_size=32,\n",
        "    num_models=10,\n",
        "    seed_start=42\n",
        "):\n",
        "    train_ind = split_col[split_col == \"Train\"].index\n",
        "    val_ind = split_col[split_col != \"Train\"].index\n",
        "\n",
        "    # train_ind, val_ind = train_test_split(list(range(1128)), test_size=0.2)\n",
        "\n",
        "    train_df = df.loc[train_ind]\n",
        "    train_X = X[train_ind, :]\n",
        "    train_y_true = (df[target_col][train_ind] * y_std + y_mean).to_numpy()\n",
        "\n",
        "    val_df = df.loc[val_ind]\n",
        "    val_X = X[val_ind, :]\n",
        "    val_y_true = (df[target_col][val_ind] * y_std + y_mean).to_numpy()\n",
        "\n",
        "    # ---- Train dataset (fit scaler) ----\n",
        "    train_dataset = MoleculeDataset(\n",
        "        train_df,\n",
        "        train_X,\n",
        "        smiles_col,\n",
        "        target_col,\n",
        "        fit_scaler=True\n",
        "    )\n",
        "\n",
        "    # ---- Validation dataset (reuse scaler) ----\n",
        "    val_dataset = MoleculeDataset(\n",
        "        val_df,\n",
        "        val_X,\n",
        "        smiles_col,\n",
        "        target_col,\n",
        "        scaler=train_dataset.scaler\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    trained_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for i in range(num_models):\n",
        "        print(f\"\\n===== Ensemble model {i+1}/{num_models} =====\")\n",
        "        set_seed(seed_start + i)\n",
        "\n",
        "        model = DA_EAGNN(\n",
        "            node_dim=11,\n",
        "            desc_dim=X.shape[1],\n",
        "            edge_dim=6,\n",
        "            hidden_dim=128,\n",
        "            dropout=0.1\n",
        "        ).to(device)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            lr=1e-3,\n",
        "            patience=20,\n",
        "            max_epochs=150\n",
        "        )\n",
        "\n",
        "        model, best_val_loss = trainer.fit(\n",
        "            train_loader,\n",
        "            val_loader\n",
        "        )\n",
        "\n",
        "        _, train_preds, _ = trainer.test(trained_loader)\n",
        "        _, val_preds, _ = trainer.test(val_loader)\n",
        "\n",
        "        train_preds_true = (train_preds * y_std + y_mean).numpy()\n",
        "\n",
        "        val_preds_true = (val_preds * y_std + y_mean).numpy()\n",
        "\n",
        "        train_rmse = np.sqrt(mean_squared_error(\n",
        "            train_preds_true,\n",
        "            train_y_true\n",
        "        ))\n",
        "\n",
        "        train_r2 = r2_score(\n",
        "            train_preds_true,\n",
        "            train_y_true\n",
        "        )\n",
        "\n",
        "        train_mae = mean_absolute_error(\n",
        "            train_preds_true,\n",
        "            train_y_true\n",
        "        )\n",
        "\n",
        "        val_rmse = np.sqrt(mean_squared_error(\n",
        "            val_preds_true,\n",
        "            val_y_true\n",
        "        ))\n",
        "\n",
        "        val_r2 = r2_score(\n",
        "            val_preds_true,\n",
        "            val_y_true\n",
        "        )\n",
        "\n",
        "        val_mae = mean_absolute_error(\n",
        "            val_preds_true,\n",
        "            val_y_true\n",
        "        )\n",
        "\n",
        "        fold_results.append({\n",
        "            \"repeat\": i + 1,\n",
        "            \"best_train_rmse\": train_rmse,\n",
        "            \"best_train_r2\": train_r2,\n",
        "            \"best_train_mae\": train_mae,\n",
        "            \"best_val_rmse\": val_rmse,\n",
        "            \"best_val_r2\": val_r2,\n",
        "            \"best_val_mae\": val_mae\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"Ensemble {i + 1} | \"\n",
        "            f\"Best Val Loss: {best_val_loss:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "    return fold_results\n"
      ],
      "metadata": {
        "id": "9HBhhL7GyDbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£8Ô∏è‚É£ Run scaffold-based ensemble training for the descriptor-augmented edge-aware GNN and save fold-wise performance metrics\n",
        "\n",
        "results = train_ensemble_scaffold(\n",
        "            df_data,\n",
        "            X,\n",
        "            df['BM-Scaffold'],\n",
        "            device,\n",
        "            smiles_col=smiles_col,\n",
        "            target_col=target_col,\n",
        "            y_mean=y_mean,\n",
        "            y_std=y_std\n",
        "        )\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"Ensemble results descriptor augmented edge aware GNN Scaffold.csv\", index=False)"
      ],
      "metadata": {
        "id": "v-cjqcyAyDv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£9Ô∏è‚É£"
      ],
      "metadata": {
        "id": "d1s-dWTGyEG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
