{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MZiaAfzal71/Edge-Aware-GNN/blob/main/Models/RF_and_XGBoost_for_ESOL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd3ySVIzQoiH"
      },
      "source": [
        "# Descriptor-Based Machine Learning Models for ESOL\n",
        "\n",
        "This notebook presents a **systematic descriptor-based modeling pipeline** for predicting\n",
        "aqueous solubility on the **ESOL (Delaney) dataset**, using classical machine learning models.\n",
        "\n",
        "The workflow is organized into three main stages:\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Hyperparameter Optimization via Repeated Cross-Validation\n",
        "\n",
        "- Hyperparameter tuning is performed using **Optuna**\n",
        "- A **5√ó5 repeated cross-validation** strategy is employed to ensure robust model selection\n",
        "- Each candidate configuration is evaluated using identical data splits\n",
        "- The optimization objective is based on cross-validated predictive performance\n",
        "\n",
        "This step is conducted **once per model**, and the resulting best hyperparameters\n",
        "are fixed for all subsequent experiments.\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Model Evaluation with Fixed Hyperparameters\n",
        "\n",
        "- Using the optimized hyperparameters, each model is re-evaluated using the same\n",
        "  **5√ó5 repeated cross-validation** scheme\n",
        "- This provides an unbiased estimate of model performance and stability\n",
        "- Mean and standard deviation of performance metrics are reported across all folds and repeats\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Scaffold-Based Ensemble Evaluation\n",
        "\n",
        "- The ESOL dataset includes a pre-defined **Bemis‚ÄìMurcko scaffold split**\n",
        "- A dedicated column (`BM-Scaffold`) specifies **Train / Validation / Test** assignments\n",
        "- Using this split:\n",
        "  - Five independent models are trained with different random seeds\n",
        "  - An **ensemble of five models** is constructed by averaging predictions\n",
        "- This setup evaluates model generalization under a chemically realistic scaffold split\n",
        "\n",
        "---\n",
        "\n",
        "## Molecular Representation\n",
        "\n",
        "- Each molecule is represented exclusively by **RDKit-computed molecular descriptors**\n",
        "- Out of 217 total RDKit descriptors:\n",
        "  - **198 descriptors with non-zero variance** are retained\n",
        "  - Constant descriptors are removed prior to modeling\n",
        "- Descriptor features are normalized using statistics computed from training data only\n",
        "\n",
        "---\n",
        "\n",
        "## Models Covered\n",
        "\n",
        "- **Random Forest Regressor**\n",
        "- **XGBoost Regressor**\n",
        "\n",
        "All experiments are conducted with an emphasis on:\n",
        "- **Reproducibility**\n",
        "- **Fair model comparison**\n",
        "- **Chemically meaningful evaluation protocols**\n",
        "\n",
        "This notebook complements graph-based modeling approaches by providing\n",
        "strong descriptor-based baselines for aqueous solubility prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RvEcRfaPXLD"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£ Fetch data\n",
        "!git clone https://github.com/MZiaAfzal71/Edge-Aware-GNN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2vKKgZ_SyKN"
      },
      "outputs": [],
      "source": [
        "# 2Ô∏è‚É£ Change current/working directory\n",
        "%cd Combined-Interaction-Matrix/ESOL\\ Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghQBihVfS32V"
      },
      "outputs": [],
      "source": [
        "# 3Ô∏è‚É£ Install rdkit and Optuna\n",
        "!pip install rdkit\n",
        "!pip install -U optuna\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsmPGTw8S8Os"
      },
      "outputs": [],
      "source": [
        "#  4Ô∏è‚É£ Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "import optuna\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold, cross_val_score, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6T6ZKsiY_N5"
      },
      "outputs": [],
      "source": [
        "# 5Ô∏è‚É£ Define dataset paths and initialize repeated k-fold cross-validation configuration\n",
        "\n",
        "file_path = \"delaney-processed-scaffold.csv\"\n",
        "smiles_col = \"smiles\"\n",
        "target_col = \"measured log solubility in mols per litre\"\n",
        "\n",
        "n_splits = 5\n",
        "n_repeats = 5\n",
        "random_state = 42\n",
        "\n",
        "CV = RepeatedKFold(\n",
        "    n_splits=n_splits,\n",
        "    n_repeats=n_repeats,\n",
        "    random_state=random_state\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ODT13mFWuW2"
      },
      "outputs": [],
      "source": [
        "# 6Ô∏è‚É£ Optuna objective function for tuning Random Forest hyperparameters using cross-validated RMSE\n",
        "\n",
        "def rf_objective(trial, X, y):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 8, 40),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
        "        \"max_features\": trial.suggest_float(\"max_features\", 0.3, 0.8),\n",
        "        \"bootstrap\": True,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "\n",
        "    model = RandomForestRegressor(**params)\n",
        "\n",
        "    rmse = -cross_val_score(\n",
        "        model,\n",
        "        X,\n",
        "        y,\n",
        "        cv=CV,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        n_jobs=-1\n",
        "    ).mean()\n",
        "\n",
        "    return rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmsYtVcfYpKh"
      },
      "outputs": [],
      "source": [
        "# 7Ô∏è‚É£ Optuna objective function for tuning XGBoost model hyperparameters using cross-validated RMSE\n",
        "\n",
        "def xgb_objective(trial, X, y):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 1200),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 0.5),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 0.2),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "        \"tree_method\": \"hist\",\n",
        "    }\n",
        "\n",
        "    model = XGBRegressor(**params)\n",
        "\n",
        "    rmse = -cross_val_score(\n",
        "        model,\n",
        "        X,\n",
        "        y,\n",
        "        cv=CV,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        n_jobs=-1\n",
        "    ).mean()\n",
        "\n",
        "    return rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQKMJlLol5O-"
      },
      "outputs": [],
      "source": [
        "# 8Ô∏è‚É£ Calculates all available RDKit descriptors for a given SMILES string.\n",
        "\n",
        "def rdkit_descriptors_from_smiles(smiles):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles : list of smiles strings\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with descriptor names as columns.\n",
        "        Returns NaN values if SMILES is invalid.\n",
        "    \"\"\"\n",
        "    # Get descriptor names and functions\n",
        "    rdkit_descs = []\n",
        "\n",
        "    desc_list = Descriptors.descList\n",
        "    desc_names = [name for name, _ in desc_list]\n",
        "\n",
        "    # Initialize output with NaNs\n",
        "    # values = [np.nan] * len(desc_names)\n",
        "\n",
        "    for sm in tqdm(smiles, total=len(smiles)):\n",
        "      sm_descs = []\n",
        "      mol = Chem.MolFromSmiles(sm)\n",
        "      if mol is None:\n",
        "          rdkit_descs.append([]*len(desc_names))\n",
        "          continue\n",
        "\n",
        "      for _, func in desc_list:\n",
        "          try:\n",
        "              sm_descs.append(func(mol))\n",
        "          except Exception:\n",
        "              sm_descs.append(np.nan)\n",
        "\n",
        "      rdkit_descs.append(sm_descs)\n",
        "\n",
        "    return pd.DataFrame(rdkit_descs, columns=desc_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2PV6swFYrbD"
      },
      "outputs": [],
      "source": [
        "# 9Ô∏è‚É£ Load dataset, compute RDKit descriptors from SMILES, remove zero-variance features, and prepare input matrix\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "y = df[target_col]\n",
        "\n",
        "# y_mean = y.mean()\n",
        "# y_std  = y.std()\n",
        "\n",
        "# y_scaled = (y - y_mean) / y_std\n",
        "\n",
        "df_data = df[[smiles_col, target_col]]# pd.concat([df[smiles_col], y], axis=1)\n",
        "\n",
        "rdkit_descriptors = rdkit_descriptors_from_smiles(df[smiles_col])\n",
        "variance_df = rdkit_descriptors.var()\n",
        "zero_var_columns = variance_df[variance_df == 0].index.tolist()\n",
        "cleaned_rdkit_descs = rdkit_descriptors.drop(columns = zero_var_columns)\n",
        "X = cleaned_rdkit_descs.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYmtYdz8mGSB"
      },
      "outputs": [],
      "source": [
        "# üîü Run Optuna hyperparameter optimization for Random Forest, report best results, and save trial history\n",
        "# This cell takes 4 hours and 19 minutes to compolete 37 out of 50 searches. And still (1:30) hours left to\n",
        "# complete the remaining 13 iterations. However, I be able to complete this search in Kaggle in  (3:15) hours\n",
        "# approximately, which uses 4 processors instead of colab (uses 2 cores).\n",
        "\n",
        "rf_study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"RF_ESOL\",\n",
        "    storage=\"sqlite:///rf_esol_optuna.db\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "rf_study.optimize(\n",
        "    lambda trial: rf_objective(trial, X, y),\n",
        "    n_trials=50,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(\"Best RF RMSE:\", rf_study.best_value)\n",
        "print(\"Best RF Params:\")\n",
        "for k, v in rf_study.best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "rf_df_trials = rf_study.trials_dataframe()\n",
        "rf_df_trials.to_csv(\"rf_study_optuna.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_u6Y9kozYtRJ"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£1Ô∏è‚É£ Run Optuna hyperparameter optimization for Random Forest, report best results, and save trial history\n",
        "\n",
        "xgb_study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"XGB_ESOL\",\n",
        "    storage=\"sqlite:///xgb_esol_optuna.db\",\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "xgb_study.optimize(\n",
        "    lambda trial: xgb_objective(trial, X, y),\n",
        "    n_trials=75,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(\"Best XGB RMSE:\", xgb_study.best_value)\n",
        "print(\"Best XGB Params:\")\n",
        "for k, v in xgb_study.best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "xgb_df_trials = xgb_study.trials_dataframe()\n",
        "xgb_df_trials.to_csv(\"xgb_study_optuna.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4SowaKqYwuS"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£2Ô∏è‚É£ Evaluate multiple regression models using cross-validation and return R¬≤, RMSE, and MAE metrics\n",
        "\n",
        "def evaluate_models(X, y, models, cv=CV):\n",
        "    \"\"\"\n",
        "    models: dict name -> sklearn-style estimator\n",
        "    returns: dict of metrics DataFrames\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    scoring = ['r2','neg_root_mean_squared_error','neg_mean_absolute_error']\n",
        "    for name, model in models.items():\n",
        "        # cross_validate\n",
        "        scores = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=True, n_jobs=-1)\n",
        "        # convert negatives back for RMSE and MAE\n",
        "        train_r2s = scores['train_r2']\n",
        "        train_rmses = -scores['train_neg_root_mean_squared_error']\n",
        "        train_maes = -scores['train_neg_mean_absolute_error']\n",
        "        val_r2s = scores['test_r2']\n",
        "        val_rmses = -scores['test_neg_root_mean_squared_error']\n",
        "        val_maes = -scores['test_neg_mean_absolute_error']\n",
        "        results[name] = {\n",
        "            'best_train_rmse': train_rmses, 'best_train_r2': train_r2s, 'best_train_mae': train_maes,\n",
        "            'best_val_rmse': val_rmses, 'best_val_r2': val_r2s, 'best_val_mae': val_maes\n",
        "        }\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsqvNJElyB3O"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£3Ô∏è‚É£ Define optimized RF and XGBoost models, evaluate them via cross-validation, and save fold-wise performance metrics\n",
        "\n",
        "rf_best_params = {\n",
        "        \"n_estimators\": 893,\n",
        "        \"max_depth\": 34,\n",
        "        \"min_samples_split\": 3,\n",
        "        \"min_samples_leaf\": 1,\n",
        "        \"max_features\": 0.3299064510871738,\n",
        "        \"bootstrap\": True,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "\n",
        "xgb_best_params = {\n",
        "        \"n_estimators\": 1195,\n",
        "        \"learning_rate\": 0.04937618493560799,\n",
        "        \"max_depth\": 3,\n",
        "        \"min_child_weight\": 1,\n",
        "        \"subsample\": 0.6251344316154072,\n",
        "        \"colsample_bytree\": 0.5938164957956602,\n",
        "        \"gamma\": 0.0032080746355426033,\n",
        "        \"reg_alpha\": 0.05691988236631272,\n",
        "        \"reg_lambda\": 6.9129621061574875,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "        \"tree_method\": \"hist\",\n",
        "    }\n",
        "\n",
        "models = {\"RF\" : RandomForestRegressor(**rf_best_params),\n",
        "          \"XGB\" : XGBRegressor(**xgb_best_params)}\n",
        "\n",
        "results = evaluate_models(X, y, models, cv=CV)\n",
        "\n",
        "rf_df = pd.DataFrame(results[\"RF\"])\n",
        "fold_ind = list(range(1, 6))*5\n",
        "rf_df.insert(loc=0, column=\"fold\", value=fold_ind)\n",
        "fold_ind.sort()\n",
        "rf_df.insert(loc=0, column=\"repeat\", value=fold_ind)\n",
        "\n",
        "xgb_df = pd.DataFrame(results[\"XGB\"])\n",
        "fold_ind = list(range(1, 6))*5\n",
        "xgb_df.insert(loc=0, column=\"fold\", value=fold_ind)\n",
        "fold_ind.sort()\n",
        "xgb_df.insert(loc=0, column=\"repeat\", value=fold_ind)\n",
        "\n",
        "rf_df.to_csv(\"Folds results Random Forest.csv\", index=False)\n",
        "xgb_df.to_csv(\"Folds results XGBoost.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CRtx9lErUBA"
      },
      "outputs": [],
      "source": [
        "split_col = df['BM-Scaffold']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixylBysVrZgY"
      },
      "outputs": [],
      "source": [
        "X[split_col[split_col != \"Train\"].index].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzPfo8Dxk59x"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£4Ô∏è‚É£ Train an ensemble of RF or XGBoost models on a scaffold split and report train/validation performance metrics\n",
        "\n",
        "def train_ensemble_scaffold(\n",
        "    X,\n",
        "    y,\n",
        "    split_col,\n",
        "    best_params,\n",
        "    model_name,\n",
        "    num_models=10,\n",
        "    seed_start=42\n",
        "):\n",
        "    train_ind = split_col[split_col == \"Train\"].index\n",
        "    val_ind = split_col[split_col != \"Train\"].index\n",
        "\n",
        "    train_X = X[train_ind]\n",
        "    train_y = y[train_ind]\n",
        "\n",
        "    val_X = X[val_ind]\n",
        "    val_y = y[val_ind]\n",
        "\n",
        "    results = {\n",
        "            'best_train_rmse': [], 'best_train_r2': [], 'best_train_mae': [],\n",
        "            'best_val_rmse': [], 'best_val_r2': [], 'best_val_mae': []\n",
        "        }\n",
        "\n",
        "    for i in range(num_models):\n",
        "        print(f\"\\n===== Ensemble model-{model_name} {i+1}/{num_models} =====\")\n",
        "        seed = (seed_start + i)\n",
        "\n",
        "        best_params[\"random_state\"] = seed\n",
        "\n",
        "        if model_name == \"RF\":\n",
        "          model = RandomForestRegressor(**best_params)\n",
        "        else:\n",
        "          model = XGBRegressor(**best_params)\n",
        "\n",
        "        model.fit(train_X, train_y)\n",
        "\n",
        "        train_pred = model.predict(train_X)\n",
        "        val_pred = model.predict(val_X)\n",
        "\n",
        "\n",
        "        train_rmse = np.sqrt(mean_squared_error(train_pred, train_y))\n",
        "        train_r2 = r2_score(train_pred, train_y)\n",
        "        train_mae = mean_absolute_error(train_pred, train_y)\n",
        "\n",
        "        val_rmse = np.sqrt(mean_squared_error(val_pred, val_y))\n",
        "        val_r2 = r2_score(val_pred, val_y)\n",
        "        val_mae = mean_absolute_error(val_pred, val_y)\n",
        "\n",
        "        results['best_train_rmse'].append(train_rmse)\n",
        "        results['best_train_r2'].append(train_r2)\n",
        "        results['best_train_mae'].append(train_mae)\n",
        "\n",
        "        results['best_val_rmse'].append(val_rmse)\n",
        "        results['best_val_r2'].append(val_r2)\n",
        "        results['best_val_mae'].append(val_mae)\n",
        "\n",
        "\n",
        "        print(f\"Test RMSE: {val_rmse} | R2 score: {val_r2} | MAE: {val_mae}\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkLBJeGrzo5i"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£5Ô∏è‚É£ Train RF and XGBoost ensemble models using scaffold-based splitting and save their performance results\n",
        "\n",
        "\n",
        "rf_results = train_ensemble_scaffold(X, y, df['BM-Scaffold'], rf_best_params, \"RF\")\n",
        "\n",
        "rf_df = pd.DataFrame(rf_results)\n",
        "\n",
        "ensemble_ind = list(range(1, 11))\n",
        "rf_df.insert(loc=0, column=\"Ensemble\", value=ensemble_ind)\n",
        "\n",
        "rf_df.to_csv(\"Ensemble results Random Forest Scaffold.csv\", index=False)\n",
        "\n",
        "xgb_results = train_ensemble_scaffold(X, y, df['BM-Scaffold'], xgb_best_params, \"XGB\")\n",
        "\n",
        "xgb_df = pd.DataFrame(xgb_results)\n",
        "\n",
        "ensemble_ind = list(range(1, 11))\n",
        "xgb_df.insert(loc=0, column=\"Ensemble\", value=ensemble_ind)\n",
        "\n",
        "xgb_df.to_csv(\"Ensemble results XGBoost Scaffold.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC-s3Z7vx51-"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£6Ô∏è‚É£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HBhhL7GyDbJ"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£7Ô∏è‚É£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-cjqcyAyDv3"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£8Ô∏è‚É£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1s-dWTGyEG1"
      },
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£9Ô∏è‚É£"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}